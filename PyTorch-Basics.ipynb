{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Basics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Tensors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch is interesting in the sense that the mathematical operations are stored in a Dynamic Computational Graph (DCG). PyTorch runs in a \"Symbolic Programming\" way. Reading in commands, storing as a DCG. Computation is done when the script is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(3, dtype=float)\n",
    "y = 5 \n",
    "w = torch.from_numpy(np.array([1,2,3])).type(torch.DoubleTensor)\n",
    "w.requires_grad = True\n",
    "y_hat = torch.dot(w, x) + 3 \n",
    "Loss = (y-y_hat) ** 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch uses the Backpropogation technique to compute the derivatives of a function. Usually the cost/loss function for a model is a sequence of operations applied to the model parameters. These operations are stored in a DCG as mentioned above. To compute the gradient of a cost function (L):\n",
    "\n",
    "- Set \"requires_grad\" attribute of the parameter tensor to true. This tells PyTorch that we will be evaluating the derivative of a function at this point. Thus we should track the operations, we will utilise the DCG through chain rule to compute the derivative. \n",
    "- Call the \"backward\" method on the loss function to output the derivative of cost at these model params. \n",
    "- The gradient of the loss is stored as the \"grad\" attribute of the **model parameters**, not the loss function.\n",
    "\n",
    "The code below shows a demonstration of this, we're expecting the gradient of this to be (8,8,8). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8., 8., 8.], dtype=torch.float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Loss.backward()\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import torch.nn as nn \n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8)\n",
      "(20640,)\n",
      "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup   Latitude  \\\n",
      "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556  37.880001   \n",
      "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842  37.860001   \n",
      "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260  37.849998   \n",
      "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945  37.849998   \n",
      "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467  37.849998   \n",
      "\n",
      "    Longitude  \n",
      "0 -122.230003  \n",
      "1 -122.220001  \n",
      "2 -122.239998  \n",
      "3 -122.250000  \n",
      "4 -122.250000  \n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = datasets.fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "# PyTorch functions with floats. \n",
    "X_train = X_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor = torch.nn.functional.normalize(torch.from_numpy(X_train.values))\n",
    "y_tensor = torch.from_numpy(y_train.values)\n",
    "y_tensor = y_tensor.view(-1, 1)\n",
    "num_features = X_tensor.shape[1]\n",
    "num_datapoints = X_tensor.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[-0.1584,  0.0747,  0.2309,  0.2794,  0.0714, -0.0722,  0.2221, -0.1983]]), tensor([-0.3461])]\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(num_features, 1)\n",
    "model_params = [t.data for t in model.parameters()]\n",
    "print(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20640, 1])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(model(X_tensor).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss for Epoch 5000 is: 1.21\n",
      "The loss for Epoch 10000 is: 1.19\n",
      "The loss for Epoch 15000 is: 1.17\n",
      "The loss for Epoch 20000 is: 1.15\n",
      "The loss for Epoch 25000 is: 1.13\n",
      "The loss for Epoch 30000 is: 1.12\n",
      "The loss for Epoch 35000 is: 1.11\n",
      "The loss for Epoch 40000 is: 1.10\n",
      "The loss for Epoch 45000 is: 1.09\n",
      "The loss for Epoch 50000 is: 1.08\n"
     ]
    }
   ],
   "source": [
    "num_iters = 50000\n",
    "learning_rate = 0.5\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(1, num_iters+1):\n",
    "    # What do we need to do in each iteration of SGD?:\n",
    "    # Feed forward, Compute loss, gradient of loss, update model params.\n",
    "\n",
    "    # Feed Forward:\n",
    "    predictions = model(X_tensor)\n",
    "\n",
    "    # Computing Loss\n",
    "    loss = loss_func(y_tensor, predictions)\n",
    "\n",
    "    # Call the backward method to compute the gradient. Recall done implicitly as model params are store in the instance. \n",
    "    loss.backward()\n",
    "\n",
    "    # Apply a step of gradient descent\n",
    "    optimizer.step()\n",
    "\n",
    "    # Set the gradient of model params to be 0. \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Printing loss every 200 iterations.\n",
    "    if epoch % 5000 == 0:\n",
    "        print(f'The loss for Epoch {epoch} is: {loss:.2f}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "742eda1bdf1e218a3bb2b4bc9aaa5c450f2dcf1623d3e127dc8dee9e2156d7e1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
